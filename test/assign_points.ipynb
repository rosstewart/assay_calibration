{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6006b-ab37-4c3b-8ba1-3a30666a0b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "logging.getLogger('matplotlib').setLevel(logging.ERROR)\n",
    "sys.path.append(\"..\")\n",
    "from src.assay_calibration.fit_utils.fit import (calculate_score_ranges,thresholds_from_prior)  # noqa: E402\n",
    "from src.assay_calibration.fit_utils.two_sample import density_utils  # noqa: E402\n",
    "from src.assay_calibration.data_utils.dataset import Scoreset  # noqa: E402\n",
    "from src.assay_calibration.fit_utils.utils import serialize_dict  # noqa: E402\n",
    "\n",
    "\n",
    "\n",
    "def summarize_scoreset(fits,scoreset,save_filepath,use_median_prior,use_2c_equation,n_c,benign_method, **kwargs):\n",
    "    \"\"\"\n",
    "    Summarizes a scoreset based on the provided arguments.\n",
    "    Args:\n",
    "        args: An object containing the following attributes:\n",
    "            - fits (List[Dict]) : List of fit results\n",
    "            - scoreset_name (str) : Name of the scoreset to summarize\n",
    "            - df (pandas.DataFrame, optional): A pandas DataFrame to be summarized. \n",
    "              If provided, this will be used directly.\n",
    "            - pillar_df_filepath (str, optional): A file path to a CSV file. If `df` \n",
    "              is not provided, the CSV file at this path will be read into a pandas \n",
    "              DataFrame and used for summarization.\n",
    "            - use_median_prior (bool). Whether to use median prior or 5-th percentile most conservative bootstrap for each threshold.\n",
    "            - use_2c_equation (bool). Whether to use prior equation rather than EM for 2c.\n",
    "    Optional Keyword Args:\n",
    "        - point_values (List[int], optional): List of point values to assign. Defaults to \n",
    "          [1,2,3,4,5,6,7,8].\n",
    "        - pathogenic_idx (int, optional): Index of the pathogenic component. Defaults to 0.\n",
    "        - benign_idx (int, optional): Index of the benign component. Defaults to 1.\n",
    "        - tolerance (float, optional): Tolerance for convergence in prior estimation. Defaults to 1e-4.\n",
    "        - max_em_steps (int, optional): Maximum number of EM steps for prior estimation. Defaults to 10000.\n",
    "\n",
    "    Note:\n",
    "        Either `df` or `pillar_df_filepath` must be provided in `args`. If both are \n",
    "        provided, `df` takes precedence.\n",
    "    \"\"\"\n",
    "    priors, prior, point_ranges, score_range, log_fp, log_fb, all_path_ranges, all_ben_ranges, C = process_fits(fits,scoreset,use_median_prior,use_2c_equation,benign_method,Path(save_filepath))\n",
    "    results = dict(prior=prior,\n",
    "                   point_ranges=point_ranges,\n",
    "                   priors=priors,\n",
    "                   score_range=score_range,\n",
    "                   log_lr_plus=log_fp - log_fb,\n",
    "                   C=C,\n",
    "                   all_path_ranges=all_path_ranges,\n",
    "                   all_ben_ranges=all_ben_ranges)\n",
    "    results = serialize_dict(results)\n",
    "    save_filepath = Path(save_filepath)\n",
    "    save_filepath.parent.mkdir(exist_ok=True,parents=True)\n",
    "    # with open(save_filepath,'w') as f:\n",
    "    #     json.dump(results,f,indent=2)\n",
    "    # save_filepath_compact = save_filepath.parent / f\"{save_filepath.stem}_compact.json\"\n",
    "    # with open(save_filepath_compact,'w') as f:\n",
    "    #     json.dump({k: results[k] for k in ['prior','point_ranges']},\n",
    "    #               f,indent=2)\n",
    "    scoreset_fit_figure = plot_scoreset(scoreset, results, fits,score_range, use_median_prior, use_2c_equation, n_c, benign_method, C)\n",
    "    figure_filepath = save_filepath.parent / f\"{save_filepath.stem}_figure_fits.png\"\n",
    "    scoreset_fit_figure.savefig(figure_filepath,bbox_inches='tight',dpi=300)\n",
    "    plt.close(scoreset_fit_figure) \n",
    "    summary_fig = plot_summary(scoreset, fits, results, score_range, log_fp, log_fb, use_median_prior, use_2c_equation, n_c, benign_method, C)\n",
    "    \n",
    "    summary_figure_filepath = save_filepath.parent / f\"{save_filepath.stem}_figure_summary.png\"\n",
    "    summary_fig.savefig(summary_figure_filepath,bbox_inches='tight',dpi=300)\n",
    "    plt.close(summary_fig)\n",
    "\n",
    "    return scoreset, results, fits, score_range, f\"({'equation' if use_2c_equation else 'em'}, {'median' if use_median_prior else '5-percentile'}, {benign_method})\", n_c\n",
    "\n",
    "def enforce_monotonicity_point_ranges(point_ranges, point_values):\n",
    "    max_path_points = None\n",
    "    max_ben_points = None\n",
    "    for i in point_values:\n",
    "        point = i # pathogenic\n",
    "\n",
    "        if max_path_points is not None:\n",
    "            point_ranges[point] = []\n",
    "        elif len(point_ranges[point]) > 1: # e.g. --_-\n",
    "            # flatten\n",
    "            point_ranges[point] = [[point_ranges[point][0][0], point_ranges[point][-1][-1]]]\n",
    "            if max_path_points is None:\n",
    "                max_path_points = point\n",
    "                \n",
    "        point = -i # benign\n",
    "\n",
    "        if max_ben_points is not None:\n",
    "            point_ranges[point] = []\n",
    "        elif len(point_ranges[point]) > 1: # e.g. --_-\n",
    "            # flatten\n",
    "            point_ranges[point] = [[point_ranges[point][0][0], point_ranges[point][-1][-1]]]\n",
    "            if max_ben_points is None:\n",
    "                max_ben_points = point\n",
    "        \n",
    "\n",
    "def prior_equation_2c(w_p, w_b, w_g):\n",
    "    return (w_g[1] - w_b[1]) / (w_p[1] - w_b[1])\n",
    "\n",
    "def prior_invalid(prior):\n",
    "    return prior <= 0 or prior >= 1\n",
    "\n",
    "def get_bootstrap_score_ranges(fitIdx, fit, fp, fb, score_range, fit_priors, point_values):\n",
    "    fit_xmin, fit_xmax = fit['fit']['xlims']\n",
    "    mask = (score_range >= fit_xmin) & (score_range <= fit_xmax)\n",
    "\n",
    "    # log_fp_local = np.zeros_like(fp)\n",
    "    # log_fb_local = np.zeros_like(fb)\n",
    "\n",
    "    # CRITICAL: IGNORE BOOTSTRAPS THAT DON'T SPAN DATA POINT. MARKING 0 WILL CAUSE STRANGE LR+ CURVES AT EXTREMES\n",
    "    log_fp_local = np.full_like(fp, np.nan, dtype=float)\n",
    "    log_fb_local = np.full_like(fb, np.nan, dtype=float)\n",
    "\n",
    "    log_fp_local[mask] = fp[mask]\n",
    "    log_fb_local[mask] = fb[mask]\n",
    "\n",
    "    lrP = log_fp_local[mask] - log_fb_local[mask]\n",
    "    s = score_range[mask]\n",
    "\n",
    "    \n",
    "    ranges_p, ranges_b, C = calculate_score_ranges(\n",
    "        lrP, lrP, fit_priors[fitIdx], s, point_values\n",
    "    )\n",
    "    \n",
    "    if prior_invalid(fit_priors[fitIdx]):\n",
    "        for key in ranges_p:\n",
    "            ranges_p[key] = [np.nan]\n",
    "        for key in ranges_b:\n",
    "            ranges_b[key] = [np.nan]\n",
    "\n",
    "    return fitIdx, log_fp_local, log_fb_local, ranges_p, ranges_b, int(C)\n",
    "\n",
    "\n",
    "def process_fits(fits, scoreset, use_median_prior, use_2c_equation, benign_method, save_filepath, **kwargs)->Tuple[np.ndarray,Dict[int,List[Tuple[float,float]]],np.ndarray,np.ndarray,np.ndarray,List[Dict[int,List[Tuple[float,float]]]],List[Dict[int,List[Tuple[float,float]]]]]:\n",
    "    n_cores = os.cpu_count() or 1\n",
    "\n",
    "    priors_filepath = save_filepath.parent / f\"{save_filepath.stem.replace('_median','').replace('_5-percentile','')}_priors.npy\"\n",
    "    if not priors_filepath.exists():\n",
    "    \n",
    "        if not use_2c_equation:\n",
    "            print('estimating priors...')\n",
    "            fit_priors = np.array(Parallel(n_jobs=min(len(fits), n_cores), verbose=10)(delayed(get_fit_prior)(fit, scoreset, benign_method, **kwargs)\n",
    "                                       for fit in fits))\n",
    "        else:\n",
    "            print('computing priors with equation...')\n",
    "            fit_priors = []\n",
    "            for fit in fits:\n",
    "                if len(fit['fit']['weights']) == 3:\n",
    "                    w_p, w_b, w_g = fit['fit']['weights']\n",
    "                elif len(fit['fit']['weights']) == 4:\n",
    "                    w_p, w_b, w_g, w_s = fit['fit']['weights']\n",
    "                else:\n",
    "                    raise ValueError(f\"Number of samples != 3 or 4: {len(fit['fit']['weights'])}\")\n",
    "                if benign_method == 'synonymous':\n",
    "                    fit_priors.append(prior_equation_2c(w_p, w_s, w_g))\n",
    "                elif benign_method == 'avg':\n",
    "                    w_bs = (np.array(w_b)+np.array(w_s))/2\n",
    "                    fit_priors.append(prior_equation_2c(w_p, w_bs, w_g))\n",
    "                else:\n",
    "                    fit_priors.append(prior_equation_2c(w_p, w_b, w_g))\n",
    "            fit_priors = np.array(fit_priors)\n",
    "            \n",
    "        np.save(priors_filepath, fit_priors)\n",
    "        \n",
    "    else:\n",
    "        print(f\"loading priors from cached {'equation' if use_2c_equation else 'em'}\")\n",
    "        fit_priors = np.load(priors_filepath)\n",
    "\n",
    "    \n",
    "    point_values = kwargs.get('point_values',[1,2,3,4,5,6,7,8])\n",
    "    # if use_median_prior:\n",
    "    prior = np.nanmedian(fit_priors)\n",
    "    # else:\n",
    "    #     prior = np.array([np.nanmin(fit_priors), np.nanmax(fit_priors)]) # set threshold on per-bootstrap basis, use fifth percentile for the most conservative thresholds\n",
    "    observed_scores = scoreset.scores[scoreset._sample_assignments.any(1)]\n",
    "    score_range = np.linspace(*np.percentile(observed_scores,[0,100]),10000) # type: ignore\n",
    "\n",
    "    _log_fp = np.stack([density_utils.mixture_pdf(score_range, _fit['fit']['component_params'],_fit['fit']['weights'][0])\n",
    "                        for _fit in fits])\n",
    "    benign_idx = 3 if benign_method == 'synonymous' else 1\n",
    "    if benign_method != 'avg':\n",
    "        _log_fb = np.stack([density_utils.mixture_pdf(score_range, _fit['fit']['component_params'],_fit['fit']['weights'][benign_idx])\n",
    "                           for _fit in fits])\n",
    "    else:\n",
    "        # print('avg b/s density:','b',fits[0]['fit']['weights'][1],'s',fits[0]['fit']['weights'][3],'bs',np.mean([fits[0]['fit']['weights'][1],fits[0]['fit']['weights'][3]]),)\n",
    "        _log_fb = np.stack([density_utils.mixture_pdf(score_range, _fit['fit']['component_params'],(np.array(_fit['fit']['weights'][1])+np.array(_fit['fit']['weights'][3]))/2)\n",
    "                           for _fit in fits])\n",
    "    log_fp = np.full((len(fits),len(score_range)),np.nan)\n",
    "    log_fb = np.full((len(fits),len(score_range)),np.nan)\n",
    "    # ranges_pathogenic = []\n",
    "    # ranges_benign = []\n",
    "\n",
    "    # print('getting thresholds for each bootstrap...')\n",
    "    # for fitIdx,(fit, fp,fb) in enumerate(zip(fits, _log_fp,_log_fb)):\n",
    "    #     fit_xmin,fit_xmax = fit['fit']['xlims']\n",
    "    #     mask = (score_range >= fit_xmin) & (score_range <= fit_xmax)\n",
    "    #     log_fp[fitIdx,mask] = fp[mask]\n",
    "    #     log_fb[fitIdx,mask] = fb[mask]\n",
    "    #     lrP = log_fp[fitIdx,mask] - log_fb[fitIdx,mask]\n",
    "    #     s = score_range[mask]\n",
    "    #     ranges_p, ranges_b = calculate_score_ranges(lrP,lrP, fit_priors[fitIdx],s,\n",
    "    #                                                 point_values) # point_ranges = {point_value : [score1, score2, ...]}\n",
    "    #     ranges_pathogenic.append(ranges_p)\n",
    "    #     ranges_benign.append(ranges_b)\n",
    "\n",
    "\n",
    "    boot_points_filepath = save_filepath.parent / f\"{save_filepath.stem.replace('_median','').replace('_5-percentile','')}_boot_points.npz\"\n",
    "    \n",
    "    # if True:# not boot_points_filepath.exists():\n",
    "    print('getting point ranges for each bootstrap...')\n",
    "    results = Parallel(\n",
    "        n_jobs=min(len(fits), n_cores),\n",
    "        verbose=10\n",
    "    )(\n",
    "        delayed(get_bootstrap_score_ranges)(fitIdx, fit, fp, fb, score_range, fit_priors, point_values)\n",
    "        for fitIdx, (fit, fp, fb) in enumerate(zip(fits, _log_fp, _log_fb))\n",
    "    )\n",
    "        # with open(boot_points_filepath, 'wb') as f:\n",
    "        #     pickle.dump(results, f)\n",
    "    # else:\n",
    "    #     print('loading cached point ranges for each bootstrap...')\n",
    "    #     with open(boot_points_filepath, 'rb') as f:\n",
    "    #         results = pickle.load(f)\n",
    "    \n",
    "    # Update parent arrays in main process\n",
    "    ranges_pathogenic, ranges_benign = [], []\n",
    "    Cs = []\n",
    "    \n",
    "    for fitIdx, log_fp_local, log_fb_local, ranges_p, ranges_b, C in results:\n",
    "        log_fp[fitIdx] = log_fp_local\n",
    "        log_fb[fitIdx] = log_fb_local\n",
    "        ranges_pathogenic.append({key: np.array(value).reshape(-1) for key, value in ranges_p.items()})\n",
    "        ranges_benign.append({key: np.array(value).reshape(-1) for key, value in ranges_b.items()})\n",
    "        Cs.append(C)\n",
    "    \n",
    "    \n",
    "        # np.savez_compressed(boot_points_filepath,\n",
    "        #             log_fp=log_fp, log_fb=log_fb, ranges_pathogenic=ranges_pathogenic, ranges_benign=ranges_benign, Cs=Cs)\n",
    "\n",
    "    # else:\n",
    "    #     print('loading cached point ranges for each bootstrap...')\n",
    "    #     boot_points_results = np.load(boot_points_filepath, allow_pickle=True)\n",
    "    #     log_fp=boot_points_results['log_fp']\n",
    "    #     log_fb=boot_points_results['log_fb']\n",
    "    #     ranges_pathogenic=boot_points_results['ranges_pathogenic']\n",
    "    #     ranges_benign=boot_points_results['ranges_benign']\n",
    "    #     Cs=boot_points_results['Cs']\n",
    "\n",
    "    log_lr_plus = log_fp - log_fb\n",
    "    nan_counts = np.isnan(log_lr_plus).sum(0)\n",
    "    range_subset = nan_counts < log_lr_plus.shape[1] # changed from 0. 1000/10000 is arbitrary\n",
    "    point_ranges = {}\n",
    "    \n",
    "    C = np.array([np.nanpercentile(Cs, 5), np.nanpercentile(Cs, 95)])\n",
    "    print('ranges_p bootstrap 0, score +1:',ranges_pathogenic[0][1])\n",
    "\n",
    "    # print('nan_counts',nan_counts, nan_counts.shape)\n",
    "    # print('range_subset',range_subset, range_subset.shape)\n",
    "    # print('log_lr_plus.shape',log_lr_plus.shape, log_lr_plus[:,0])\n",
    "    \n",
    "    # if points 1 and -1 are out of order, consider scoreset flipped\n",
    "    scoreset_flipped = len(ranges_pathogenic[0][1]) != 0 and len(ranges_benign[0][-1]) != 0 and ranges_pathogenic[0][1][-1] >= ranges_benign[0][-1][0]\n",
    "    \n",
    "    if prior > 0 and prior < 1:\n",
    "        \n",
    "        if use_median_prior:\n",
    "            print('using median prior to get unified thresholds...')\n",
    "            # point_ranges = {point_value : [score1, score2, ...]}\n",
    "            point_ranges_pathogenic, point_ranges_benign, C = calculate_score_ranges(np.nanpercentile(log_lr_plus[:,range_subset],\n",
    "                                                                                                5,axis=0),\n",
    "                                                                                    np.nanpercentile(log_lr_plus[:,range_subset],\n",
    "                                                                                                    95,axis=0),\n",
    "                                                                                    prior,\n",
    "                                                                                    score_range[range_subset],\n",
    "                                                                                    point_values,)\n",
    "            point_ranges = {**point_ranges_pathogenic,**point_ranges_benign}\n",
    "        else:\n",
    "            # use 5-percentile of most conservative bootstrap thresholds for each point assignment\n",
    "            print('using 5-percentile to get conservative thresholds...')\n",
    "            \n",
    "            # print('ranges_benign[0]',ranges_benign[0])\n",
    "            # print('scoreset_flipped:',scoreset_flipped,len(ranges_pathogenic[0][1]) != 0 , len(ranges_benign[0][-1]) != 0 , ranges_pathogenic[0][1][-1] >= ranges_benign[0][-1][0])\n",
    "            p_5percentile_conservative = 5 if not scoreset_flipped else 95\n",
    "            b_5percentile_conservative = 95 if not scoreset_flipped else 5\n",
    "            p_max = max if not scoreset_flipped else min\n",
    "            b_min = min if not scoreset_flipped else max\n",
    "            p_inf = -np.inf if not scoreset_flipped else np.inf\n",
    "            b_inf = np.inf if not scoreset_flipped else -np.inf\n",
    "\n",
    "            conservative_thresholds = {}\n",
    "            # print('ranges_pathogenic',ranges_pathogenic[0])\n",
    "            # print('ranges_benign',ranges_benign[0])\n",
    "            print('boot prior:',np.nanmin(fit_priors), '-', np.nanmax(fit_priors))\n",
    "            for point_value in point_values: # 1,2,...,8\n",
    "\n",
    "                conservative_thresholds[point_value] = np.nanpercentile([p_max(ranges_p[point_value]) if len(ranges_p[point_value]) > 0 else p_inf for ranges_p in ranges_pathogenic], p_5percentile_conservative)\n",
    "                print(point_value,'nan bootstrap points:',np.isnan([p_max(ranges_p[point_value]) if len(ranges_p[point_value]) > 0 else p_inf for ranges_p in ranges_pathogenic]).sum())\n",
    "                conservative_thresholds[-1*point_value] = np.nanpercentile([b_min(ranges_b[-1*point_value]) if len(ranges_b[-1*point_value]) > 0 else b_inf for ranges_b in ranges_benign], b_5percentile_conservative)\n",
    "\n",
    "            print('conservative_thresholds',conservative_thresholds)\n",
    "            for point_value, threshold in conservative_thresholds.items():\n",
    "                assert point_value != 0\n",
    "                if np.isnan(threshold) or np.isinf(threshold):\n",
    "                    point_ranges[point_value] = []\n",
    "                    continue\n",
    "                valid_scores = score_range[range_subset]\n",
    "                if (point_value > 0 and not scoreset_flipped) or (point_value < 0 and scoreset_flipped): # pathogenic or flipped benign\n",
    "                    if abs(point_value) == max(point_values):\n",
    "                        point_ranges[point_value] = [valid_scores[0], threshold]\n",
    "                    else:\n",
    "                        lower_lim = conservative_thresholds[point_value+1] if point_value > 0 else conservative_thresholds[point_value-1]\n",
    "                        if np.isnan(lower_lim):\n",
    "                            point_ranges[point_value] = [valid_scores[0], threshold]\n",
    "                        else:\n",
    "                            # point_ranges[point_value] = valid_scores[(valid_scores > lower_lim) & (valid_scores < threshold)]\n",
    "                            point_ranges[point_value] = [lower_lim, threshold]\n",
    "                else: # benign\n",
    "                    if abs(point_value) == max(point_values):\n",
    "                        point_ranges[point_value] = [threshold, valid_scores[-1]]\n",
    "                    else:\n",
    "                        upper_lim = conservative_thresholds[point_value-1] if point_value < 0 else conservative_thresholds[point_value+1]\n",
    "                        if np.isnan(upper_lim):\n",
    "                            point_ranges[point_value] = [threshold, valid_scores[-1]]\n",
    "                        else:\n",
    "                            # point_ranges[point_value] = valid_scores[(valid_scores < upper_lim) & (valid_scores > threshold)]   \n",
    "                            point_ranges[point_value] = [threshold, upper_lim]\n",
    "                \n",
    "            for point_value in point_ranges:\n",
    "                if len(point_ranges[point_value]) != 0:\n",
    "                    point_ranges[point_value] = [point_ranges[point_value]]\n",
    "            print('point_ranges',point_ranges)\n",
    "    \n",
    "    # enforce point range monotonicty before returning\n",
    "    enforce_monotonicity_point_ranges(point_ranges, point_values)\n",
    "    \n",
    "    return fit_priors, prior, point_ranges,score_range[range_subset],log_fp[:,range_subset], log_fb[:,range_subset], ranges_pathogenic, ranges_benign, C\n",
    "    \n",
    "\n",
    "\n",
    "def get_fit_prior(fit, scoreset, benign_method, **kwargs):\n",
    "    pathogenic_idx = kwargs.get('pathogenic_idx',0)\n",
    "    benign_idx = kwargs.get('benign_idx',1)\n",
    "    if benign_method == 'synonymous':\n",
    "        benign_idx = 3\n",
    "    params = fit['fit']['component_params']\n",
    "    weights = fit['fit']['weights']\n",
    "    population = scoreset.scores[scoreset._sample_assignments[:,2]]\n",
    "    pathogenic_density = density_utils.joint_densities(population,\n",
    "                                                       params,\n",
    "                                                       weights[pathogenic_idx]).sum(axis=0)\n",
    "    if benign_method != 'avg':\n",
    "        benign_density = density_utils.joint_densities(population,\n",
    "                                                       params,\n",
    "                                                       weights[benign_idx]).sum(axis=0)\n",
    "    else:\n",
    "        bs_weights = (np.array(weights[1])+np.array(weights[3]))/2\n",
    "        benign_density = density_utils.joint_densities(population,\n",
    "                                                       params,\n",
    "                                                       bs_weights).sum(axis=0)\n",
    "        \n",
    "    assert len(pathogenic_density) == len(population)\n",
    "    assert len(benign_density) == len(population)\n",
    "    prior_estimate = 0.5\n",
    "    converged = False\n",
    "    em_steps = 0\n",
    "    max_em_steps = kwargs.get(\"max_em_steps\",10000)\n",
    "    while not converged:\n",
    "        em_steps += 1\n",
    "\n",
    "        with np.errstate(divide='ignore', invalid='ignore', over='ignore',under='ignore'):\n",
    "            posteriors = 1 / (\n",
    "                1\n",
    "                + (1 - prior_estimate)\n",
    "                / prior_estimate\n",
    "                * benign_density # type: ignore\n",
    "                / pathogenic_density\n",
    "            )\n",
    "        new_prior = np.nanmean(posteriors)\n",
    "        prior_estimate = new_prior\n",
    "        if prior_estimate < 0 or prior_estimate > 1:\n",
    "            raise ValueError(f\"Invalid prior estimate obtained, {prior_estimate}\")\n",
    "        if em_steps >= max_em_steps:\n",
    "            break\n",
    "    return prior_estimate\n",
    "    \n",
    "def plot_scoreset(scoreset:Scoreset, summary: Dict, scoreset_fits: List[Dict], score_range, use_median_prior,use_2c_equation, n_c, benign_method, C):\n",
    "    fig, ax = plt.subplots(2,scoreset.n_samples, figsize=(5*scoreset.n_samples,10),sharex=True,sharey=False)\n",
    "    for sample_num in range(scoreset.n_samples):\n",
    "        sns.histplot(scoreset.scores[scoreset.sample_assignments[:,sample_num]],stat='density',ax=ax[1,sample_num],alpha=.5,color='pink',)\n",
    "        density = sample_density(score_range, scoreset_fits, sample_num)\n",
    "        for compNum in range(density.shape[1]):\n",
    "\n",
    "            compDensity = density[:,compNum,:]\n",
    "            d = np.nanpercentile(compDensity,[5,50,95],axis=0)\n",
    "            ax[1,sample_num].plot(score_range,d[1],color=f\"C{compNum}\",linestyle='--',label=f\"Comp {compNum+1}\")\n",
    "            ax[1,sample_num].legend()\n",
    "        d = np.nansum(density,axis=1)\n",
    "        d_perc = np.percentile(d,[5,50,95],axis=0)\n",
    "        ax[1,sample_num].plot(score_range,d_perc[1],color='black',alpha=.5)\n",
    "        ax[1,sample_num].fill_between(score_range,d_perc[0],d_perc[2],color='gray',alpha=0.3)\n",
    "        ax[1,sample_num].set_xlabel(\"Score\")\n",
    "        ax[0,sample_num].set_title(f\"{scoreset.sample_names[sample_num]} (n={scoreset.sample_assignments[:,sample_num].sum():,d})\")\n",
    "    point_ranges = sorted([(int(k), v) for k,v in summary['point_ranges'].items()])\n",
    "    point_values = [pr[0] for pr in point_ranges]\n",
    "    print(point_ranges)\n",
    "    for axi in ax[0]:\n",
    "        for pointIdx,(pointVal, scoreRanges) in enumerate(point_ranges):\n",
    "            for sr in scoreRanges:\n",
    "                axi.plot([sr[0], sr[1]], [pointIdx,pointIdx], color='red' if pointVal > 0 else 'blue', linestyle='-', alpha=0.7)\n",
    "        axi.set_ylim(-1,len(point_values))\n",
    "        axi.set_ylabel(\"Points\")\n",
    "\n",
    "        axi.set_yticks(range(len(point_values)),labels=list(map(lambda i: f\"{i:+d}\" if i!=0 else \"0\",point_values)))\n",
    "    ax[0,2].set_title(f\"{scoreset.scoreset_name} ({n_c}, median:{use_median_prior},em:{not use_2c_equation}): (gnomAD pop, n={scoreset.sample_assignments[:,2].sum():,d})\\nprior {summary['prior']:.3f}, C: {summary['C']}\")\n",
    "    return fig\n",
    "\n",
    "def plot_scoreset_compare_point_assignments(dataset, scoresets, summary, scoreset_fits, score_ranges, n_samples):\n",
    "    \n",
    "    # Get 2c and 3c scoresets\n",
    "    scoreset_2c = scoresets[list(scoresets.keys())[0]]  # 2c with samples\n",
    "    scoreset_3c = scoresets[list(scoresets.keys())[-1]]  # 3c with samples\n",
    "    score_range_2c = score_ranges[list(score_ranges.keys())[0]]\n",
    "    score_range_3c = score_ranges[list(score_ranges.keys())[-1]]\n",
    "    \n",
    "    # Get configs - SORT CONSISTENTLY\n",
    "    configs_2c = sorted([k for k in summary.keys() if k[1] == '2c' and 'avg' not in k]) + \\\n",
    "                 sorted([k for k in summary.keys() if k[1] == '2c' and 'avg' in k])\n",
    "    \n",
    "    configs_3c = sorted([k for k in summary.keys() if k[1] == '3c' and 'avg' not in k]) + \\\n",
    "                 sorted([k for k in summary.keys() if k[1] == '3c' and 'avg' in k])\n",
    "        \n",
    "    # Determine actual number of sample columns needed\n",
    "    n_samples_2c = scoreset_2c.n_samples\n",
    "    n_samples_3c = scoreset_3c.n_samples\n",
    "    max_samples = max(n_samples_2c, n_samples_3c)\n",
    "    \n",
    "    # Layout: 4 rows (3c lr+, 3c fits/points, 2c fits/points, 2c lr+)\n",
    "    # Columns: max(n_samples) + max(n_configs)\n",
    "    max_configs = max(len(configs_2c), len(configs_3c))\n",
    "    n_cols_total = max_samples + max_configs\n",
    "    \n",
    "    fig, ax = plt.subplots(4, n_cols_total, figsize=(5*n_cols_total, 20), \n",
    "                           squeeze=False, gridspec_kw={'hspace': 0.3, 'wspace': 0.3})\n",
    "\n",
    "    # ===== Row 0: 3c LR+ summaries =====\n",
    "    # Hide sample columns in row 0\n",
    "    for col_idx in range(max_samples):\n",
    "        ax[0, col_idx].axis('off')\n",
    "    \n",
    "    # Will get xlim after plotting fits\n",
    "    \n",
    "    # ===== Row 1: 3c fits and point assignments =====\n",
    "    # Plot 3c fits\n",
    "    for sample_num in range(n_samples_3c):\n",
    "        ax_fit = ax[1, sample_num]\n",
    "        \n",
    "        sns.histplot(scoreset_3c.scores[scoreset_3c.sample_assignments[:,sample_num]], \n",
    "                     stat='density', ax=ax_fit, alpha=.5, color='pink')\n",
    "        \n",
    "        density = sample_density(score_range_3c, scoreset_fits[list(scoreset_fits.keys())[-1]], sample_num)\n",
    "        for compNum in range(density.shape[1]):\n",
    "            compDensity = density[:,compNum,:]\n",
    "            d = np.nanpercentile(compDensity,[5,50,95],axis=0)\n",
    "            ax_fit.plot(score_range_3c, d[1], color=f\"C{compNum}\", linestyle='--', label=f\"Comp {compNum+1}\")\n",
    "        ax_fit.legend(fontsize=8)\n",
    "        \n",
    "        d = np.nansum(density, axis=1)\n",
    "        d_perc = np.percentile(d, [5,50,95], axis=0)\n",
    "        ax_fit.plot(score_range_3c, d_perc[1], color='black', alpha=.5)\n",
    "        ax_fit.fill_between(score_range_3c, d_perc[0], d_perc[2], color='gray', alpha=0.3)\n",
    "        ax_fit.set_title(f\"3c: {scoreset_3c.sample_names[sample_num]}\\n(n={scoreset_3c.sample_assignments[:,sample_num].sum():,d})\")\n",
    "        ax_fit.set_xlabel(\"Score\")\n",
    "        ax_fit.set_ylabel(\"Density\")\n",
    "        ax_fit.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused sample columns for 3c in row 1\n",
    "    for col_idx in range(n_samples_3c, max_samples):\n",
    "        ax[1, col_idx].axis('off')\n",
    "    \n",
    "    # Get x-limits from 3c fits\n",
    "    xlim_3c = ax[1, 0].get_xlim()\n",
    "    \n",
    "    # Now plot 3c LR+ summaries in row 0 (now that we have xlim)\n",
    "    for config_idx, (config, n_c) in enumerate(configs_3c):\n",
    "        col_idx = max_samples + config_idx\n",
    "        ax_lr = ax[0, col_idx]\n",
    "        \n",
    "        log_lr_plus = summary[(config, n_c)]['log_lr_plus']\n",
    "        llr_curves = np.nanpercentile(np.array(log_lr_plus),[5,50,95],axis=0)\n",
    "        labels = ['5th percentile','Median','95th percentile']\n",
    "        \n",
    "        for i, c in enumerate(['red','black','blue']):\n",
    "            ax_lr.plot(score_range_3c, llr_curves[i], color=c, label=labels[i])\n",
    "        \n",
    "        point_values = sorted(list(set([abs(int(k)) for k in summary[(config, n_c)]['point_ranges'].keys()])))\n",
    "        tauP, tauB, _ = list(map(np.log, thresholds_from_prior(summary[(config, n_c)]['prior'], point_values)))\n",
    "        priors = np.percentile(np.array(summary[(config, n_c)]['priors']),[5,50,95])\n",
    "        \n",
    "        ax_lr.set_title(f\"3c LR+ {config}\\nprior: {priors[1]:.3f} ({priors[0]:.3f}-{priors[2]:.3f}), C: {summary[(config, n_c)]['C']}\", fontsize=10)\n",
    "        add_thresholds(tauP, tauB, ax_lr)\n",
    "        ax_lr.set_xlabel(\"Score\")\n",
    "        ax_lr.set_ylabel(\"Log LR+\")\n",
    "        ax_lr.legend(fontsize=6, loc='best')\n",
    "        ax_lr.set_xlim(xlim_3c)\n",
    "        ax_lr.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused config columns for 3c in row 0\n",
    "    for col_idx in range(max_samples + len(configs_3c), n_cols_total):\n",
    "        ax[0, col_idx].axis('off')\n",
    "    \n",
    "    # Plot 3c point assignments in row 1\n",
    "    for config_idx, (config, n_c) in enumerate(configs_3c):\n",
    "        col_idx = max_samples + config_idx\n",
    "        ax_points = ax[1, col_idx]\n",
    "        \n",
    "        point_ranges = sorted([(int(k), v) for k,v in summary[(config, n_c)]['point_ranges'].items()])\n",
    "        point_values = [pr[0] for pr in point_ranges]\n",
    "        \n",
    "        # Plot all samples on same axis\n",
    "        for sample_num in range(n_samples_3c):\n",
    "            for pointIdx, (pointVal, scoreRanges) in enumerate(point_ranges):\n",
    "                for sr in scoreRanges:\n",
    "                    ax_points.plot([sr[0], sr[1]], [pointIdx, pointIdx], \n",
    "                                 color='red' if pointVal > 0 else 'blue', \n",
    "                                 linestyle='-', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax_points.set_ylim(-1, len(point_values))\n",
    "        ax_points.set_yticks(range(len(point_values)), \n",
    "                           labels=list(map(lambda i: f\"{i:+d}\" if i!=0 else \"0\", point_values)))\n",
    "        ax_points.set_xlabel(\"Score\")\n",
    "        ax_points.set_ylabel(\"Points\")\n",
    "        ax_points.set_title(f\"3c Points {config}\", fontsize=10)\n",
    "        ax_points.set_xlim(xlim_3c)\n",
    "        ax_points.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused config columns for 3c in row 1\n",
    "    for col_idx in range(max_samples + len(configs_3c), n_cols_total):\n",
    "        ax[1, col_idx].axis('off')\n",
    "\n",
    "    \n",
    "    # ===== Row 2: 2c fits and point assignments =====\n",
    "    # Plot 2c fits\n",
    "    for sample_num in range(n_samples_2c):\n",
    "        ax_fit = ax[2, sample_num]\n",
    "        \n",
    "        sns.histplot(scoreset_2c.scores[scoreset_2c.sample_assignments[:,sample_num]], \n",
    "                     stat='density', ax=ax_fit, alpha=.5, color='pink')\n",
    "        \n",
    "        density = sample_density(score_range_2c, scoreset_fits[list(scoreset_fits.keys())[0]], sample_num)\n",
    "        for compNum in range(density.shape[1]):\n",
    "            compDensity = density[:,compNum,:]\n",
    "            d = np.nanpercentile(compDensity,[5,50,95],axis=0)\n",
    "            ax_fit.plot(score_range_2c, d[1], color=f\"C{compNum}\", linestyle='--', label=f\"Comp {compNum+1}\")\n",
    "        ax_fit.legend(fontsize=8)\n",
    "        \n",
    "        d = np.nansum(density, axis=1)\n",
    "        d_perc = np.percentile(d, [5,50,95], axis=0)\n",
    "        ax_fit.plot(score_range_2c, d_perc[1], color='black', alpha=.5)\n",
    "        ax_fit.fill_between(score_range_2c, d_perc[0], d_perc[2], color='gray', alpha=0.3)\n",
    "        ax_fit.set_title(f\"2c: {scoreset_2c.sample_names[sample_num]}\\n(n={scoreset_2c.sample_assignments[:,sample_num].sum():,d})\")\n",
    "        ax_fit.set_xlabel(\"Score\")\n",
    "        ax_fit.set_ylabel(\"Density\")\n",
    "        ax_fit.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused sample columns for 2c in row 2\n",
    "    for col_idx in range(n_samples_2c, max_samples):\n",
    "        ax[2, col_idx].axis('off')\n",
    "    \n",
    "    # Get x-limits from 2c fits\n",
    "    xlim_2c = ax[2, 0].get_xlim()\n",
    "    \n",
    "    # Plot 2c point assignments in row 2\n",
    "    for config_idx, (config, n_c) in enumerate(configs_2c):\n",
    "        col_idx = max_samples + config_idx\n",
    "        ax_points = ax[2, col_idx]\n",
    "        \n",
    "        point_ranges = sorted([(int(k), v) for k,v in summary[(config, n_c)]['point_ranges'].items()])\n",
    "        point_values = [pr[0] for pr in point_ranges]\n",
    "        \n",
    "        # Plot all samples on same axis\n",
    "        for sample_num in range(n_samples_2c):\n",
    "            for pointIdx, (pointVal, scoreRanges) in enumerate(point_ranges):\n",
    "                for sr in scoreRanges:\n",
    "                    ax_points.plot([sr[0], sr[1]], [pointIdx, pointIdx], \n",
    "                                 color='red' if pointVal > 0 else 'blue', \n",
    "                                 linestyle='-', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax_points.set_ylim(-1, len(point_values))\n",
    "        ax_points.set_yticks(range(len(point_values)), \n",
    "                           labels=list(map(lambda i: f\"{i:+d}\" if i!=0 else \"0\", point_values)))\n",
    "        ax_points.set_xlabel(\"Score\")\n",
    "        ax_points.set_ylabel(\"Points\")\n",
    "        ax_points.set_title(f\"2c Points {config}\", fontsize=10)\n",
    "        ax_points.set_xlim(xlim_2c)\n",
    "        ax_points.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused config columns for 2c in row 2\n",
    "    for col_idx in range(max_samples + len(configs_2c), n_cols_total):\n",
    "        ax[2, col_idx].axis('off')\n",
    "    \n",
    "    # ===== Row 3: 2c LR+ summaries =====\n",
    "    # Hide sample columns in row 3\n",
    "    for col_idx in range(max_samples):\n",
    "        ax[3, col_idx].axis('off')\n",
    "    \n",
    "    for config_idx, (config, n_c) in enumerate(configs_2c):\n",
    "        col_idx = max_samples + config_idx\n",
    "        ax_lr = ax[3, col_idx]\n",
    "        \n",
    "        log_lr_plus = summary[(config, n_c)]['log_lr_plus']\n",
    "        llr_curves = np.nanpercentile(np.array(log_lr_plus),[5,50,95],axis=0)\n",
    "        labels = ['5th percentile','Median','95th percentile']\n",
    "        \n",
    "        for i, c in enumerate(['red','black','blue']):\n",
    "            ax_lr.plot(score_range_2c, llr_curves[i], color=c, label=labels[i])\n",
    "        \n",
    "        point_values = sorted(list(set([abs(int(k)) for k in summary[(config, n_c)]['point_ranges'].keys()])))\n",
    "        tauP, tauB, _ = list(map(np.log, thresholds_from_prior(summary[(config, n_c)]['prior'], point_values)))\n",
    "        priors = np.percentile(np.array(summary[(config, n_c)]['priors']),[5,50,95])\n",
    "        \n",
    "        ax_lr.set_title(f\"2c LR+ {config}\\nprior: {priors[1]:.3f} ({priors[0]:.3f}-{priors[2]:.3f}), C: {summary[(config, n_c)]['C']}\", fontsize=10)\n",
    "        add_thresholds(tauP, tauB, ax_lr)\n",
    "        ax_lr.set_xlabel(\"Score\")\n",
    "        ax_lr.set_ylabel(\"Log LR+\")\n",
    "        ax_lr.legend(fontsize=6, loc='best')\n",
    "        ax_lr.set_xlim(xlim_2c)\n",
    "        ax_lr.grid(linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Hide unused config columns for 2c in row 3\n",
    "    for col_idx in range(max_samples + len(configs_2c), n_cols_total):\n",
    "        ax[3, col_idx].axis('off')\n",
    "    \n",
    "    fig.suptitle(f\"{scoreset_2c.scoreset_name}\", fontsize=16, y=0.995)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def sample_density(x, fits, sampleNum):\n",
    "    _density = np.stack([density_utils.joint_densities(x, _fit['fit']['component_params'],_fit['fit']['weights'][sampleNum])\n",
    "                        for _fit in fits])\n",
    "    density = np.full(_density.shape,np.nan)\n",
    "    for fitIdx,fit in enumerate(fits):\n",
    "        fit_xmin,fit_xmax = fit['fit']['xlims']\n",
    "        mask = (x >= fit_xmin) & (x <= fit_xmax)\n",
    "        density[fitIdx,:,mask] = _density[fitIdx,:,mask]\n",
    "    return density\n",
    "\n",
    "def add_thresholds(tauP, tauB, ax):\n",
    "    for tp,tb in zip(tauP,tauB):\n",
    "        ax.axhline(tp,color='red',linestyle='--',alpha=0.5)\n",
    "        ax.axhline(tb,color='blue',linestyle='--',alpha=0.5)\n",
    "\n",
    "def plot_summary(scoreset: Scoreset, fits: List[Dict], summary:Dict, score_range, log_fp, log_fb, use_median_prior,use_2c_equation, n_c, benign_method, C):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
    "    log_lr_plus = log_fp - log_fb\n",
    "    llr_curves = np.nanpercentile(np.array(log_lr_plus),[5,50,95],axis=0)\n",
    "    labels = ['5th percentile','Median','95th percentile']\n",
    "    for i,c in enumerate(['red','black','blue']):\n",
    "        ax.plot(score_range,llr_curves[i],color=c,label=labels[i])\n",
    "    point_values = sorted(list(set([abs(int(k)) for k in summary['point_ranges'].keys()])))\n",
    "    tauP,tauB,_ = list(map(np.log, thresholds_from_prior(summary['prior'],point_values)) )\n",
    "    priors = np.percentile(np.array(summary['priors']),[5,50,95])\n",
    "    ax.set_title(f\"{dataset} ({n_c}, median:{use_median_prior},em:{not use_2c_equation}): prior: {priors[1]:.3f} ({priors[0]:.3f}-{priors[2]:.3f}), C: {C}\")\n",
    "    add_thresholds(tauP, tauB, ax)\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Log LR+\")\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d6613e-55dd-4506-9878-bf33b490799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dir = f'/data/ross/assay_calibration/point_assignments/{\"un\" if not constrained else \"\"}constrained_pngs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e20f9a4-1f76-470e-b076-386cd36d4bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "constrained = True\n",
    "\n",
    "if constrained:\n",
    "    results_name = 'initial_datasets_results_1000bootstraps_100fits'\n",
    "    with gzip.open(f'/data/ross/assay_calibration/{results_name}.json.gz', 'rt', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    results_name = 'clinvar_circ_datasets_results_1000bootstraps_100fits'\n",
    "    with gzip.open(f'/data/ross/assay_calibration/{results_name}.json.gz', 'rt', encoding='utf-8') as f:\n",
    "        results = {**results, **json.load(f)}\n",
    "\n",
    "else:\n",
    "    results_name = 'unconstrained_rerun_initial_datasets_results_1000bootstraps_100fits'\n",
    "    with gzip.open(f'/data/ross/assay_calibration/{results_name}.json.gz', 'rt', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "urgent_datasets=(\n",
    "  'VHL_Buckley_2024',\n",
    "  'XRCC2_unpublished',\n",
    "  'BARD1_unpublished'\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "241a732a-b379-4dca-8a8b-278e7c687577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ross/assay_calibration/point_assignment_comparison/ASPA_Grønbæk-Thygesen_2024_abundance/ASPA_Grønbæk-Thygesen_2024_abundance_2c_5-percentile_em.pkl exists\n",
      "Starting ASPA_Grønbæk-Thygesen_2024_abundance_2c_5-percentile_em...\n",
      "loading priors from cached em\n",
      "getting point ranges for each bootstrap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=72)]: Using backend LokyBackend with 72 concurrent workers.\n",
      "[Parallel(n_jobs=72)]: Done   1 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=72)]: Done  18 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=72)]: Done  37 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=72)]: Done  56 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=72)]: Done  77 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=72)]: Done  98 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=72)]: Done 121 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=72)]: Done 144 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=72)]: Done 169 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=72)]: Done 194 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=72)]: Done 221 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=72)]: Done 248 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=72)]: Done 277 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=72)]: Done 306 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=72)]: Done 337 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=72)]: Done 368 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=72)]: Done 401 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=72)]: Done 434 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=72)]: Done 469 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=72)]: Done 504 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=72)]: Done 541 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=72)]: Done 578 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=72)]: Done 617 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=72)]: Done 656 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=72)]: Done 697 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=72)]: Done 738 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=72)]: Done 781 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=72)]: Done 824 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=72)]: Done 958 out of 1000 | elapsed:   21.9s remaining:    1.0s\n",
      "[Parallel(n_jobs=72)]: Done 1000 out of 1000 | elapsed:   22.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges_p bootstrap 0, score +1: [nan]\n",
      "using 5-percentile to get conservative thresholds...\n",
      "boot prior: 0.0 - 1.0\n",
      "1 nan bootstrap points: 128\n",
      "2 nan bootstrap points: 128\n",
      "3 nan bootstrap points: 128\n",
      "4 nan bootstrap points: 128\n",
      "5 nan bootstrap points: 128\n",
      "6 nan bootstrap points: 128\n",
      "7 nan bootstrap points: 128\n",
      "8 nan bootstrap points: 128\n",
      "conservative_thresholds {1: np.float64(nan), -1: np.float64(nan), 2: np.float64(nan), -2: np.float64(nan), 3: np.float64(nan), -3: np.float64(nan), 4: np.float64(nan), -4: np.float64(nan), 5: np.float64(nan), -5: np.float64(nan), 6: np.float64(nan), -6: np.float64(nan), 7: np.float64(nan), -7: np.float64(nan), 8: np.float64(nan), -8: np.float64(nan)}\n",
      "point_ranges {1: [], -1: [], 2: [], -2: [], 3: [], -3: [], 4: [], -4: [], 5: [], -5: [], 6: [], -6: [], 7: [], -7: [], 8: [], -8: []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcstewart/miniconda3/envs/assay_calibration/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4653: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, []), (-1, []), (1, []), (2, []), (3, []), (4, []), (5, []), (6, []), (7, []), (8, [])]\n",
      "/data/ross/assay_calibration/point_assignment_comparison/ASPA_Grønbæk-Thygesen_2024_abundance/ASPA_Grønbæk-Thygesen_2024_abundance_2c_median_em.pkl exists\n",
      "Starting ASPA_Grønbæk-Thygesen_2024_abundance_2c_median_em...\n",
      "loading priors from cached em\n",
      "getting point ranges for each bootstrap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=72)]: Using backend LokyBackend with 72 concurrent workers.\n",
      "[Parallel(n_jobs=72)]: Done   1 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=72)]: Done  18 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=72)]: Done  37 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=72)]: Done  56 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=72)]: Done  77 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=72)]: Done  98 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=72)]: Done 121 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=72)]: Done 144 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=72)]: Done 169 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=72)]: Done 194 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=72)]: Done 221 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=72)]: Done 248 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=72)]: Done 277 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=72)]: Done 306 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=72)]: Done 337 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=72)]: Done 368 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=72)]: Done 401 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=72)]: Done 434 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=72)]: Done 469 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=72)]: Done 504 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=72)]: Done 541 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=72)]: Done 578 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=72)]: Done 617 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=72)]: Done 656 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=72)]: Done 697 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=72)]: Done 738 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=72)]: Done 781 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=72)]: Done 824 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=72)]: Done 958 out of 1000 | elapsed:   20.7s remaining:    0.9s\n",
      "[Parallel(n_jobs=72)]: Done 1000 out of 1000 | elapsed:   21.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges_p bootstrap 0, score +1: [nan]\n",
      "using median prior to get unified thresholds...\n",
      "[(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, []), (-1, []), (1, []), (2, []), (3, []), (4, []), (5, []), (6, []), (7, []), (8, [])]\n",
      "/data/ross/assay_calibration/point_assignment_comparison/ASPA_Grønbæk-Thygesen_2024_abundance/ASPA_Grønbæk-Thygesen_2024_abundance_2c_5-percentile_equation.pkl exists\n",
      "Starting ASPA_Grønbæk-Thygesen_2024_abundance_2c_5-percentile_equation...\n",
      "loading priors from cached equation\n",
      "getting point ranges for each bootstrap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=72)]: Using backend LokyBackend with 72 concurrent workers.\n",
      "[Parallel(n_jobs=72)]: Done   1 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=72)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=72)]: Done  37 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=72)]: Done  56 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=72)]: Done  77 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=72)]: Done  98 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=72)]: Done 121 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=72)]: Done 144 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=72)]: Done 169 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=72)]: Done 194 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=72)]: Done 221 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=72)]: Done 248 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=72)]: Done 277 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=72)]: Done 306 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=72)]: Done 337 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=72)]: Done 368 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=72)]: Done 401 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=72)]: Done 434 tasks      | elapsed:    9.8s\n",
      "[Parallel(n_jobs=72)]: Done 469 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=72)]: Done 504 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=72)]: Done 541 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=72)]: Done 578 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=72)]: Done 617 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=72)]: Done 656 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=72)]: Done 697 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=72)]: Done 738 tasks      | elapsed:   16.0s\n",
      "[Parallel(n_jobs=72)]: Done 781 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=72)]: Done 824 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=72)]: Done 958 out of 1000 | elapsed:   20.4s remaining:    0.9s\n",
      "[Parallel(n_jobs=72)]: Done 1000 out of 1000 | elapsed:   21.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges_p bootstrap 0, score +1: [nan]\n",
      "using 5-percentile to get conservative thresholds...\n",
      "boot prior: -579.1938730023005 - 48.12336582343781\n",
      "1 nan bootstrap points: 187\n",
      "2 nan bootstrap points: 187\n",
      "3 nan bootstrap points: 187\n",
      "4 nan bootstrap points: 187\n",
      "5 nan bootstrap points: 187\n",
      "6 nan bootstrap points: 187\n",
      "7 nan bootstrap points: 187\n",
      "8 nan bootstrap points: 187\n",
      "conservative_thresholds {1: np.float64(0.1285883628362836), -1: np.float64(nan), 2: np.float64(0.11961321332133215), -2: np.float64(nan), 3: np.float64(0.10673923792379239), -3: np.float64(nan), 4: np.float64(nan), -4: np.float64(nan), 5: np.float64(nan), -5: np.float64(nan), 6: np.float64(nan), -6: np.float64(nan), 7: np.float64(nan), -7: np.float64(nan), 8: np.float64(nan), -8: np.float64(nan)}\n",
      "point_ranges {1: [[np.float64(0.11961321332133215), np.float64(0.1285883628362836)]], -1: [], 2: [[np.float64(0.10673923792379239), np.float64(0.11961321332133215)]], -2: [], 3: [[np.float64(-0.0304), np.float64(0.10673923792379239)]], -3: [], 4: [], -4: [], 5: [], -5: [], 6: [], -6: [], 7: [], -7: [], 8: [], -8: []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rcstewart/miniconda3/envs/assay_calibration/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4653: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, []), (-1, []), (1, [[0.11961321332133215, 0.1285883628362836]]), (2, [[0.10673923792379239, 0.11961321332133215]]), (3, [[-0.0304, 0.10673923792379239]]), (4, []), (5, []), (6, []), (7, []), (8, [])]\n",
      "/data/ross/assay_calibration/point_assignment_comparison/ASPA_Grønbæk-Thygesen_2024_abundance/ASPA_Grønbæk-Thygesen_2024_abundance_3c_5-percentile_em.pkl exists\n",
      "Starting ASPA_Grønbæk-Thygesen_2024_abundance_3c_5-percentile_em...\n",
      "loading priors from cached em\n",
      "getting point ranges for each bootstrap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=72)]: Using backend LokyBackend with 72 concurrent workers.\n",
      "[Parallel(n_jobs=72)]: Done   1 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=72)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=72)]: Done  37 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=72)]: Done  56 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=72)]: Done  77 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=72)]: Done  98 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=72)]: Done 121 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=72)]: Done 144 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=72)]: Done 169 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=72)]: Done 194 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=72)]: Done 221 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=72)]: Done 248 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=72)]: Done 277 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=72)]: Done 306 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=72)]: Done 337 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=72)]: Done 368 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=72)]: Done 401 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=72)]: Done 434 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=72)]: Done 469 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=72)]: Done 504 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=72)]: Done 541 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=72)]: Done 578 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=72)]: Done 617 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=72)]: Done 656 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=72)]: Done 697 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=72)]: Done 738 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=72)]: Done 781 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=72)]: Done 824 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=72)]: Done 958 out of 1000 | elapsed:   20.2s remaining:    0.9s\n",
      "[Parallel(n_jobs=72)]: Done 1000 out of 1000 | elapsed:   21.1s finished\n",
      "/home/rcstewart/miniconda3/envs/assay_calibration/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4653: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges_p bootstrap 0, score +1: [0.07779896 0.07884562]\n",
      "using 5-percentile to get conservative thresholds...\n",
      "boot prior: 0.0 - 1.0\n",
      "1 nan bootstrap points: 16\n",
      "2 nan bootstrap points: 16\n",
      "3 nan bootstrap points: 16\n",
      "4 nan bootstrap points: 16\n",
      "5 nan bootstrap points: 16\n",
      "6 nan bootstrap points: 16\n",
      "7 nan bootstrap points: 16\n",
      "8 nan bootstrap points: 16\n",
      "conservative_thresholds {1: np.float64(0.07851200020002001), -1: np.float64(0.7877188948894889), 2: np.float64(0.07539817281728174), -2: np.float64(nan), 3: np.float64(0.07253946994699471), -3: np.float64(nan), 4: np.float64(0.06920322632263227), -4: np.float64(nan), 5: np.float64(0.0635577787778778), -5: np.float64(nan), 6: np.float64(0.05385650565056506), -6: np.float64(nan), 7: np.float64(0.033623169316931704), -7: np.float64(nan), 8: np.float64(nan), -8: np.float64(nan)}\n",
      "point_ranges {1: [[np.float64(0.07539817281728174), np.float64(0.07851200020002001)]], -1: [[np.float64(0.7877188948894889), np.float64(1.2778)]], 2: [[np.float64(0.07253946994699471), np.float64(0.07539817281728174)]], -2: [], 3: [[np.float64(0.06920322632263227), np.float64(0.07253946994699471)]], -3: [], 4: [[np.float64(0.0635577787778778), np.float64(0.06920322632263227)]], -4: [], 5: [[np.float64(0.05385650565056506), np.float64(0.0635577787778778)]], -5: [], 6: [[np.float64(0.033623169316931704), np.float64(0.05385650565056506)]], -6: [], 7: [[np.float64(-0.0304), np.float64(0.033623169316931704)]], -7: [], 8: [], -8: []}\n",
      "[(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, []), (-1, [[0.7877188948894889, 1.2778]]), (1, [[0.07539817281728174, 0.07851200020002001]]), (2, [[0.07253946994699471, 0.07539817281728174]]), (3, [[0.06920322632263227, 0.07253946994699471]]), (4, [[0.0635577787778778, 0.06920322632263227]]), (5, [[0.05385650565056506, 0.0635577787778778]]), (6, [[0.033623169316931704, 0.05385650565056506]]), (7, [[-0.0304, 0.033623169316931704]]), (8, [])]\n",
      "/data/ross/assay_calibration/point_assignment_comparison/ASPA_Grønbæk-Thygesen_2024_abundance/ASPA_Grønbæk-Thygesen_2024_abundance_3c_median_em.pkl exists\n",
      "Starting ASPA_Grønbæk-Thygesen_2024_abundance_3c_median_em...\n",
      "loading priors from cached em\n",
      "getting point ranges for each bootstrap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=72)]: Using backend LokyBackend with 72 concurrent workers.\n",
      "[Parallel(n_jobs=72)]: Done   1 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=72)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=72)]: Done  37 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=72)]: Done  56 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=72)]: Done  77 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=72)]: Done  98 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=72)]: Done 121 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=72)]: Done 144 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=72)]: Done 169 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=72)]: Done 194 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=72)]: Done 221 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=72)]: Done 248 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=72)]: Done 277 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=72)]: Done 306 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=72)]: Done 337 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=72)]: Done 368 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=72)]: Done 401 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=72)]: Done 434 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=72)]: Done 469 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=72)]: Done 504 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=72)]: Done 541 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=72)]: Done 578 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=72)]: Done 617 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=72)]: Done 656 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=72)]: Done 697 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=72)]: Done 738 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=72)]: Done 781 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=72)]: Done 824 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=72)]: Done 958 out of 1000 | elapsed:   25.0s remaining:    1.1s\n",
      "[Parallel(n_jobs=72)]: Done 1000 out of 1000 | elapsed:   26.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranges_p bootstrap 0, score +1: [0.07779896 0.07884562]\n",
      "using median prior to get unified thresholds...\n",
      "[(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, []), (-1, []), (1, []), (2, []), (3, []), (4, []), (5, []), (6, []), (7, []), (8, [])]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "start = False\n",
    "for dataset in results.keys(): # PRIORITIZE URGENT DATASETS TO RUN FIRST, REST ALPHANUMERIC\n",
    "\n",
    "    if dataset != 'ASPA_Grønbæk-Thygesen_2024_abundance':\n",
    "        continue\n",
    "\n",
    "    # if dataset == 'BARD1_unpublished':\n",
    "    #     start = True\n",
    "    #     continue\n",
    "    # if not start:\n",
    "        continue\n",
    "\n",
    "    dataset_f = f\"/data/ross/assay_calibration/scoresets/{dataset}.json\"\n",
    "    scoreset = Scoreset.from_json(dataset_f)\n",
    "    n_samples = len([s for s in scoreset.samples])\n",
    "\n",
    "    save_dir = f'/data/ross/assay_calibration/point_assignment_comparison/{dataset}'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    scoresets_dict, summary_dict, scoreset_fits_dict, score_ranges_dict = {},{},{},{}\n",
    "    \n",
    "    for n_c in ('2c','3c'):\n",
    "        \n",
    "        boot_results = [results[dataset][key][n_c] for key,val in results[dataset].items()]\n",
    "        # boot_results = boot_results[:20] # for speed of dev\n",
    "\n",
    "        for benign_method in ['benign', 'avg']:\n",
    "            for use_median_prior, use_2c_equation in zip([False, True, False], [False, False, True]):\n",
    "                    \n",
    "                # only do synonymous analysis if synonymous exists and only for one config\n",
    "                if benign_method != 'benign' and (not use_median_prior or use_2c_equation or n_samples != 4):\n",
    "                    continue\n",
    "                    \n",
    "                if n_c == '3c' and use_2c_equation: # N/A\n",
    "                    continue\n",
    "    \n",
    "                experiment_code = f'{dataset}_{n_c}_{\"median\" if use_median_prior else \"5-percentile\"}_{\"equation\" if use_2c_equation else \"em\"}{\"_\"+benign_method if benign_method != \"benign\" else \"\"}'\n",
    "                \n",
    "                save_filepath = f'{save_dir}/{experiment_code}.json'\n",
    "                pkl_filepath = f'{save_dir}/{experiment_code}.pkl'\n",
    "        \n",
    "                if os.path.exists(pkl_filepath):\n",
    "                    print(f'{pkl_filepath} exists')\n",
    "                    with open(pkl_filepath,'rb') as f:\n",
    "                        scoreset, indv_summary, fits, score_range, config, n_c = pickle.load(f)\n",
    "                        \n",
    "                    if not config.startswith('(e'): # old config, swap\n",
    "                        config = f\"({'equation' if use_2c_equation else 'em'}, {'median' if use_median_prior else '5-percentile'}, {benign_method})\"\n",
    "                    \n",
    "                    if 'C' in indv_summary:\n",
    "                        scoresets_dict[(config,n_c)] = scoreset\n",
    "                        summary_dict[(config,n_c)] = indv_summary\n",
    "                        scoreset_fits_dict[(config,n_c)] = fits\n",
    "                        score_ranges_dict[(config,n_c)] = score_range\n",
    "                        # continue ### rerun everthing\n",
    "                    \n",
    "                print(f'Starting {experiment_code}...')\n",
    "                try:\n",
    "                \n",
    "                    scoreset, indv_summary, fits, score_range, config, _ = summarize_scoreset(boot_results,scoreset,save_filepath,use_median_prior,use_2c_equation,n_c,benign_method)\n",
    "                    del indv_summary['all_path_ranges']\n",
    "                    del indv_summary['all_ben_ranges']\n",
    "                    \n",
    "                    scoresets_dict[(config,n_c)] = scoreset\n",
    "                    summary_dict[(config,n_c)] = indv_summary\n",
    "                    scoreset_fits_dict[(config,n_c)] = fits\n",
    "                    score_ranges_dict[(config,n_c)] = score_range\n",
    "            \n",
    "                    with open(pkl_filepath, 'wb') as f:\n",
    "                        pickle.dump((scoreset, indv_summary, fits, score_range, config, n_c), f)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "\n",
    "    point_comparison_figure = plot_scoreset_compare_point_assignments(dataset, scoresets_dict, summary_dict, scoreset_fits_dict, score_ranges_dict, n_samples)\n",
    "    \n",
    "    figure_filepath = f\"{save_dir}/{dataset}_point_comparison.png\"\n",
    "    point_comparison_figure.savefig(figure_filepath,bbox_inches='tight',dpi=300)\n",
    "    plt.close(point_comparison_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3d308-898c-4e9d-867c-d90e73274935",
   "metadata": {},
   "outputs": [],
   "source": [
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0449d7-e65f-444c-b741-476c3d1ebd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [(-8, []), (-7, []), (-6, []), (-5, []), (-4, []), (-3, []), (-2, [[-0.18066820274493, 1.1121633402781734]]), (-1, [[-0.2190387471559836, -0.18066820274493]]), (1, [[-0.9107317610725785, -0.8994764013786694]]), (2, [[-0.92352194254293, -0.9107317610725785]]), (3, [[-0.9409165893426077, -0.92352194254293]]), (4, [[-0.9961701732945247, -0.9409165893426077]]), (5, [[-3.3065885540987723, -0.9961701732945247]]), (6, []), (7, []), (8, [])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assay_calibration)",
   "language": "python",
   "name": "assay_calibration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
