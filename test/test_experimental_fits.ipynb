{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import importlib\n",
    "import src.assay_calibration.fit_utils.two_sample.fit\n",
    "from src.assay_calibration.fit_utils.fit import Fit\n",
    "importlib.reload(src.assay_calibration.fit_utils.two_sample.fit)\n",
    "importlib.reload(src.assay_calibration.fit_utils.fit)\n",
    "from src.assay_calibration.fit_utils.two_sample.fit import single_fit\n",
    "from src.assay_calibration.fit_utils.two_sample import (density_utils,constraints, optimize)\n",
    "import scipy.stats as sps\n",
    "import matplotlib\n",
    "matplotlib.set_loglevel(\"warning\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import trange\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "from src.assay_calibration.data_utils.dataset_clinvar_version_functionality import (\n",
    "    PillarProjectDataframe,\n",
    "    Scoreset,\n",
    "    BasicScoreset,\n",
    ")\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old clinvar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/ross/assay_calibration/dataframe_expanded.csv.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Dataset', 'Gene', 'HGNC_id', 'Chrom', 'STRAND', 'hg19_pos',\n",
       "       'hg38_start', 'hg38_end', 'ref_allele', 'alt_allele',\n",
       "       'auth_transcript_id', 'transcript_pos', 'transcript_ref',\n",
       "       'transcript_alt', 'aa_pos', 'aa_ref', 'aa_alt', 'hgvs_c', 'hgvs_p',\n",
       "       'consequence', 'simplified_consequence', 'auth_reported_score',\n",
       "       'auth_reported_rep_score', 'auth_reported_func_class', 'splice_measure',\n",
       "       'gnomad_MAF', 'clinvar_sig_2025', 'clinvar_star_2025',\n",
       "       'clinvar_date_last_reviewed_2025', 'nucleotide_or_aa',\n",
       "       'MaveDB Score Set URN', 'Ensembl_transcript_ID',\n",
       "       'Ref_seq_transcript_ID', 'Model_system', 'Assay Type',\n",
       "       'Phenotype Measured ontology term',\n",
       "       'Molecular or Biological Process Investigated (GO term)',\n",
       "       'IGVF_produced', 'Interval 1 name', 'Interval 1 range',\n",
       "       'Interval 1 MaveDB class', 'Interval 2 name', 'Interval 2 range',\n",
       "       'Interval 2 MaveDB class', 'Interval 3 name', 'Interval 3 range',\n",
       "       'Interval 3 MaveDB class', 'Interval 4 name', 'Interval 4 range',\n",
       "       'Interval 4 MaveDB class', 'Interval 5 name', 'Interval 5 range',\n",
       "       'Interval 5 MaveDB class', 'Interval 6 name', 'Interval 6 range',\n",
       "       'Interval 6 MaveDB class', 'Flag', 'REVEL', 'AM_score', 'AM_class',\n",
       "       'spliceAI_DS_AG', 'spliceAI_DS_AL', 'spliceAI_DS_DG', 'spliceAI_DS_DL',\n",
       "       'spliceAI_DP_AG', 'spliceAI_DP_AL', 'spliceAI_DP_DG', 'spliceAI_DP_DL',\n",
       "       'REVEL_train', 'MutPred2', 'MP2_train', 'clinvar_sig_2018',\n",
       "       'clinvar_star_2018', 'clinvar_date_last_reviewed_2018',\n",
       "       'ClinVar Variation Id_ClinGen_repo', 'Allele Registry Id_ClinGen_repo',\n",
       "       'Disease_ClinGen_repo', 'Mondo Id_ClinGen_repo',\n",
       "       'Mode of Inheritance_ClinGen_repo', 'Assertion_ClinGen_repo',\n",
       "       'Applied Evidence Codes (Met)_ClinGen_repo',\n",
       "       'Applied Evidence Codes (Not Met)_ClinGen_repo',\n",
       "       'Summary of interpretation_ClinGen_repo',\n",
       "       'PubMed Articles_ClinGen_repo', 'Expert Panel_ClinGen_repo',\n",
       "       'Guideline_ClinGen_repo', 'Approval Date_ClinGen_repo',\n",
       "       'Published Date_ClinGen_repo', 'Retracted_ClinGen_repo',\n",
       "       'Evidence Repo Link_ClinGen_repo', 'Uuid_ClinGen_repo',\n",
       "       'Updated_Classification_ClinGen_repo',\n",
       "       'Updated_Evidence Codes_ClinGen_repo'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Dataset == 'BRCA1_Findlay_2018'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAP1_Waters_2024: 18108 total variants\n",
      "\tPathogenic/Likely Pathogenic: 33 variants\n",
      "\tBenign/Likely Benign: 84 variants\n",
      "\tpopulation: 1369 variants\n",
      "\tSynonymous: 1854 variants\n",
      "\n",
      "BRCA1_Adamovich_2022_Cisplatin: 5153 total variants\n",
      "\tPathogenic/Likely Pathogenic: 33 variants\n",
      "\tBenign/Likely Benign: 10 variants\n",
      "\tpopulation: 119 variants\n",
      "\n",
      "BRCA1_Adamovich_2022_HDR: 8148 total variants\n",
      "\tPathogenic/Likely Pathogenic: 36 variants\n",
      "\tBenign/Likely Benign: 13 variants\n",
      "\tpopulation: 152 variants\n",
      "\n",
      "BRCA1_Findlay_2018: 3893 total variants\n",
      "\tPathogenic/Likely Pathogenic: 258 variants\n",
      "\tBenign/Likely Benign: 90 variants\n",
      "\tpopulation: 599 variants\n",
      "\tSynonymous: 544 variants\n",
      "\n",
      "BRCA2_Hu_2024: 462 total variants\n",
      "\tPathogenic/Likely Pathogenic: 19 variants\n",
      "\tBenign/Likely Benign: 19 variants\n",
      "\tpopulation: 243 variants\n",
      "\n",
      "BRCA2_Sahu_2023_exon13_Cisplatin: 251 total variants\n",
      "\tPathogenic/Likely Pathogenic: 14 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 46 variants\n",
      "\tSynonymous: 39 variants\n",
      "\n",
      "BRCA2_Sahu_2023_exon13_global_score: 252 total variants\n",
      "\tPathogenic/Likely Pathogenic: 14 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 46 variants\n",
      "\tSynonymous: 39 variants\n",
      "\n",
      "BRCA2_Sahu_2023_exon13_Olaparib: 251 total variants\n",
      "\tPathogenic/Likely Pathogenic: 14 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 46 variants\n",
      "\tSynonymous: 39 variants\n",
      "\n",
      "BRCA2_Sahu_2023_exon13_SGE: 251 total variants\n",
      "\tPathogenic/Likely Pathogenic: 14 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 46 variants\n",
      "\tSynonymous: 39 variants\n",
      "\n",
      "BRCA2_Sahu_2025_HDR: 6550 total variants\n",
      "\tPathogenic/Likely Pathogenic: 245 variants\n",
      "\tBenign/Likely Benign: 29 variants\n",
      "\tpopulation: 938 variants\n",
      "\tSynonymous: 1276 variants\n",
      "\n",
      "BRCA2_unpublished: 795 total variants\n",
      "\tPathogenic/Likely Pathogenic: 36 variants\n",
      "\tBenign/Likely Benign: 8 variants\n",
      "\tpopulation: 107 variants\n",
      "\tSynonymous: 147 variants\n",
      "\n",
      "DDX3X_Radford_2023_cLFC_day15: 9080 total variants\n",
      "\tPathogenic/Likely Pathogenic: 59 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 524 variants\n",
      "\tSynonymous: 1245 variants\n",
      "\n",
      "PTEN_Matreyek_2018: 14674 total variants\n",
      "\tPathogenic/Likely Pathogenic: 75 variants\n",
      "\tBenign/Likely Benign: 2 variants\n",
      "\tpopulation: 145 variants\n",
      "\tSynonymous: 150 variants\n",
      "\n",
      "PTEN_Mighell_2018: 17453 total variants\n",
      "\tPathogenic/Likely Pathogenic: 108 variants\n",
      "\tBenign/Likely Benign: 1 variants\n",
      "\tpopulation: 186 variants\n",
      "\n",
      "RAD51C_Olvera-León_2024_z_score_D4_D14: 8836 total variants\n",
      "\tPathogenic/Likely Pathogenic: 49 variants\n",
      "\tBenign/Likely Benign: 60 variants\n",
      "\tpopulation: 792 variants\n",
      "\tSynonymous: 908 variants\n",
      "\n",
      "VHL_Buckley_2024: 2268 total variants\n",
      "\tPathogenic/Likely Pathogenic: 95 variants\n",
      "\tBenign/Likely Benign: 10 variants\n",
      "\tpopulation: 344 variants\n",
      "\tSynonymous: 375 variants\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "jan 2018 for all brca datasets and p10\n",
    "vhl bap1 dtx 3x rad51c 2023 version or earliest of those paper\n",
    "'''\n",
    "# import re\n",
    "\n",
    "# def parse_decade_year(text: str):\n",
    "#     match = re.search(r'(20[0-2]\\d)', text)  # matches 2000–2029\n",
    "#     if match:\n",
    "#         return int(match.group(1))\n",
    "\n",
    "scoresets_old_clinvar = [\n",
    "    'BAP1_Waters_2024',\n",
    "    'BRCA1_Adamovich_2022_Cisplatin',\n",
    "    'BRCA1_Adamovich_2022_HDR',\n",
    "    'BRCA1_Findlay_2018',\n",
    "    'BRCA2_Hu_2024',\n",
    "    'BRCA2_Sahu_2023_exon13_Cisplatin',\n",
    "    'BRCA2_Sahu_2023_exon13_global_score',\n",
    "    'BRCA2_Sahu_2023_exon13_Olaparib',\n",
    "    'BRCA2_Sahu_2023_exon13_SGE',\n",
    "    'BRCA2_Sahu_2025_HDR',\n",
    "    'BRCA2_unpublished',\n",
    "    'DDX3X_Radford_2023_cLFC_day15',\n",
    "    'PTEN_Matreyek_2018',\n",
    "    'PTEN_Mighell_2018',\n",
    "    'RAD51C_Olvera-León_2024_z_score_D4_D14',\n",
    "    'VHL_Buckley_2024',\n",
    "]\n",
    "\n",
    "min_clinvar_star = 1\n",
    "\n",
    "for scoreset_name in scoresets_old_clinvar:\n",
    "    # clinvar_release = parse_decade_year(scoreset_name)\n",
    "    clinvar_release = '2018'\n",
    "\n",
    "    scoreset_df = df[df.Dataset == scoreset_name]\n",
    "    scoreset = Scoreset(scoreset_df,\n",
    "                        clinvar_release=clinvar_release,\n",
    "                        min_clinvar_star=min_clinvar_star)\n",
    "    scoreset.to_json(f'/data/ross/assay_calibration/scoresets_old_clinvar/{scoreset_name}_clinvar_2018.json')\n",
    "    print(scoreset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resume regular fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit(ds, component_range, check_monotonic, init_strategy):\n",
    "    # Load directly from JSON\n",
    "    fit = Fit(ds)\n",
    "    fits, _, _ = fit.run(\n",
    "                core_limit=10,\n",
    "                num_fits=20,\n",
    "                verbose_level=20,\n",
    "                component_range=component_range,\n",
    "                bootstrap=False,\n",
    "                verbose=False,\n",
    "                max_em_iters=10000,\n",
    "                check_convergence=False,\n",
    "                check_monotonic=check_monotonic,\n",
    "                # submerge_steps=None,#256 if check_monotonic else None,\n",
    "                init_strategy=init_strategy,\n",
    "                score_min=ds.scores.min() - 1,\n",
    "                score_max=ds.scores.max() + 1,\n",
    "                init_constraint_adjustment_param=\"skew\",\n",
    "                # kmean_init=\"k-means++\"\n",
    "            )\n",
    "    \n",
    "    return fits, ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DDX3X_Radford_2023_cLFC_day15\n",
      "DDX3X_Radford_2023_cLFC_day15: 9080 total variants\n",
      "\tPathogenic/Likely Pathogenic: 201 variants\n",
      "\tBenign/Likely Benign: 88 variants\n",
      "\tgnomAD: 524 variants\n",
      "\tSynonymous: 1245 variants\n",
      "\tAll Missense SNV: 4380 variants\n",
      "\n",
      "DDX3X_Radford_2023_cLFC_day15 2-component constraint [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "DDX3X_Radford_2023_cLFC_day15 2-component no constraint [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "DDX3X_Radford_2023_cLFC_day15 3-component constraint [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "DDX3X_Radford_2023_cLFC_day15 3-component no constraint [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "  Saved to /data/ross/assay_calibration/test_experimental_plots_5sample/DDX3X_Radford_2023_cLFC_day15.png\n",
      "All plots saved!\n"
     ]
    }
   ],
   "source": [
    "#### import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/data/ross/assay_calibration/test_experimental_plots_v12\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Suppress matplotlib debug messages\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "n_non_violating_fits_without_constraint = 0\n",
    "# start = False\n",
    "five_sample = False\n",
    "\n",
    "dataset_fs = glob.glob(\"/data/ross/assay_calibration/scoresets/DDX*_missense_replace_gnomad_modified*.json\")\n",
    "# dataset_fs = [f for f in dataset_fs if \"_missense_replace_gnomad_modified\" not in f]\n",
    "for dataset_f in (dataset_fs):\n",
    "    \n",
    "    dataset_name = dataset_f.split('/')[-1][:-5]\n",
    "    # if dataset_name == \"HMBS_van_Loggerenberg_2023_combined\":\n",
    "    #     start = True\n",
    "    # if not start:\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Processing {dataset_name}\")\n",
    "\n",
    "    ds = Scoreset.from_json(dataset_f, five_sample=five_sample)\n",
    "    print(ds)\n",
    "\n",
    "    sample_names = np.array([sample[1] for sample in ds.samples])\n",
    "    \n",
    "    # Create figure with 4x3 subplot grid\n",
    "    fig, axes = plt.subplots(len(sample_names), 4, figsize=(18, 5*len(sample_names)))\n",
    "    fig.suptitle(f'Dataset: {dataset_name}', fontsize=16, y=0.995)\n",
    "\n",
    "    all_results[dataset_name] = {}\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for component_range in [\"2-component\", \"3-component\"]:\n",
    "        for monotonicity_constraint in [\"constraint\", \"no constraint\"]:\n",
    "            # for init_density_fix in [\"lambda\",\"sigma\"]:\n",
    "            init_strategy = \"random\"\n",
    "            \n",
    "            # Run the fit\n",
    "            try:\n",
    "                fits, ds = test_fit(\n",
    "                    ds, \n",
    "                    component_range=[3] if component_range[0] == \"3\" else [2], \n",
    "                    check_monotonic=False if monotonicity_constraint[0] == \"n\" else True, \n",
    "                    init_strategy=init_strategy\n",
    "                )\n",
    "                \n",
    "                # Get best fit\n",
    "                # print(fits[0])\n",
    "                fit_results = sorted(fits, key=lambda res: res['likelihoods'][-1], reverse=True)\n",
    "                times_submerged = [results['times_submerged'] for results in fit_results]\n",
    "                print(dataset_name, component_range, monotonicity_constraint, times_submerged)\n",
    "                best_fit = fit_results[0]\n",
    "                best_init = \"MoM\" if best_fit['kmeans'] == \"method_of_moments\" else \"KM\"\n",
    "                xlims = best_fit['xlims']\n",
    "                scores = ds.scores\n",
    "                sample_assignments = ds.sample_assignments\n",
    "\n",
    "                \n",
    "                all_results[dataset_name][(component_range, monotonicity_constraint)] = fit_results \n",
    "    \n",
    "                # Calculate densities\n",
    "                score_range = np.linspace(scores.min(), scores.max(), 1000)\n",
    "                estimatedDensities = np.array([\n",
    "                    density_utils.joint_densities(\n",
    "                        score_range[..., None],\n",
    "                        best_fit['component_params'],\n",
    "                        sample_weights\n",
    "                    ).squeeze() for sample_weights in best_fit['weights']\n",
    "                ])\n",
    "                \n",
    "                # Check if density constraint violated\n",
    "                fit_violates_constraint = constraints.multicomponent_density_constraint_violated(best_fit['component_params'], xlims)\n",
    "                if monotonicity_constraint == \"no constraint\" and fit_violates_constraint:\n",
    "                    n_non_violating_fits_without_constraint += 1\n",
    "                fit_violates_constraint = \"violates\" if fit_violates_constraint else \"not violates\"\n",
    "\n",
    "                sample_lls = density_utils.get_sample_likelihood(scores, sample_assignments, best_fit['component_params'], best_fit['weights']) / np.array([sum(each_sample_assignments) for each_sample_assignments in sample_assignments.T])\n",
    "                \n",
    "                # Plot for each sample (3 columns)\n",
    "                for i in range(len(estimatedDensities)):\n",
    "                    ax = axes[i, plot_idx]\n",
    "    \n",
    "                    \n",
    "                    # Plot estimated density\n",
    "                    ax.plot(score_range, estimatedDensities[i].sum(0), \n",
    "                           label='Estimated', color='C1', linestyle='-', linewidth=2)\n",
    "                    \n",
    "                    # Plot histogram of actual data\n",
    "                    max_hist_height = 1.0\n",
    "                    if i < sample_assignments.shape[1] and sample_assignments[:, i].sum() > 0:\n",
    "                        sample_data = scores[sample_assignments[:, i]]\n",
    "                        counts, bins, patches = ax.hist(sample_data, \n",
    "                                                        bins=30, density=True, alpha=0.3, \n",
    "                                                        color='gray', label='Data')\n",
    "                        \n",
    "                        # Get max height of histogram\n",
    "                        max_hist_height = counts.max()\n",
    "                        \n",
    "                        # Set ylim to 1.2x the histogram max to avoid components with 1e99 density and no scale\n",
    "                        ax.set_ylim(0, max_hist_height * 1.2)\n",
    "                    \n",
    "                    # Set labels and title\n",
    "                    if plot_idx == 0:\n",
    "                        ax.set_ylabel(sample_names[i], \n",
    "                                     fontsize=12, fontweight='bold')\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        ax.set_title(f'{component_range}, {monotonicity_constraint}, {best_init}, {fit_violates_constraint}', fontsize=12)\n",
    "                    \n",
    "                    if i == len(estimatedDensities) - 1:\n",
    "                        ax.set_xlabel('Score', fontsize=10)\n",
    "                    \n",
    "                    ax.legend(loc='upper right', fontsize=8)\n",
    "                    ax.grid(True, alpha=0.2)\n",
    "                    \n",
    "                    # Add likelihood value as text\n",
    "                    likelihood = best_fit['likelihoods'][-1]\n",
    "                    ax.text(0.02, 0.98, f'SLL: {sample_lls[i]:.3f}, LL: {likelihood:.3f}, n={len(scores[sample_assignments[:, i]])}', \n",
    "                           transform=ax.transAxes, fontsize=12,\n",
    "                           verticalalignment='top')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {component_range}, {monotonicity_constraint}: {e}\")\n",
    "                # Create empty plots with error message\n",
    "                for i in range(len(estimatedDensities)):\n",
    "                    ax = axes[i, plot_idx]\n",
    "                    ax.text(0.5, 0.5, f'Error:\\n{str(e)[:30]}...', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax.set_title(f'{component_range}, {monotonicity_constraint}, {best_init}, {fit_violates_constraint}', fontsize=12)\n",
    "                    \n",
    "                    if plot_idx == 0:\n",
    "                        ax.set_ylabel(sample_names[i], \n",
    "                                     fontsize=12, fontweight='bold')\n",
    "                    \n",
    "                    if i == len(estimatedDensities) - 1:\n",
    "                        ax.set_xlabel('Score', fontsize=10)\n",
    "            \n",
    "            plot_idx += 1\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    \n",
    "    output_path = f\"{output_dir}/{dataset_name}.png\"\n",
    "    plt.savefig(output_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Saved to {output_path}\")\n",
    "\n",
    "print(\"All plots saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assay_calibration)",
   "language": "python",
   "name": "assay_calibration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
