{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import importlib\n",
    "import src.assay_calibration.fit_utils.two_sample.fit\n",
    "from src.assay_calibration.fit_utils.fit import Fit\n",
    "importlib.reload(src.assay_calibration.fit_utils.two_sample.fit)\n",
    "importlib.reload(src.assay_calibration.fit_utils.fit)\n",
    "from src.assay_calibration.fit_utils.two_sample.fit import single_fit\n",
    "from src.assay_calibration.fit_utils.two_sample import (density_utils,constraints, optimize)\n",
    "import scipy.stats as sps\n",
    "import matplotlib\n",
    "matplotlib.set_loglevel(\"warning\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import trange\n",
    "import os\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "from src.assay_calibration.data_utils.dataset import (\n",
    "    PillarProjectDataframe,\n",
    "    Scoreset,\n",
    "    BasicScoreset,\n",
    ")\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.assay_calibration.data_utils.dataset import (\n",
    "#     PillarProjectDataframe,\n",
    "#     Scoreset,\n",
    "#     BasicScoreset,\n",
    "# )\n",
    "# ds = Scoreset.from_json(\"/data/ross/assay_calibration/scoresets/CALM1_CALM2_CALM3_Weile_2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dir(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.scoreset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in ds.samples:\n",
    "#     print(sample[1])\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fit(ds, component_range, check_monotonic, init_strategy):\n",
    "    # Load directly from JSON\n",
    "    fit = Fit(ds)\n",
    "    fits = fit.run(\n",
    "                core_limit=10,\n",
    "                num_fits=10,\n",
    "                verbose_level=20,\n",
    "                component_range=component_range,\n",
    "                bootstrap=False,\n",
    "                verbose=False,\n",
    "                max_em_iters=10000,\n",
    "                check_convergence=False,\n",
    "                check_monotonic=check_monotonic,\n",
    "                # submerge_steps=None,#256 if check_monotonic else None,\n",
    "                init_strategy=init_strategy,\n",
    "                score_min=ds.scores.min() - 1,\n",
    "                score_max=ds.scores.max() + 1,\n",
    "                # kmean_init=\"k-means++\"\n",
    "            )\n",
    "    \n",
    "    return fits, ds\n",
    "\n",
    "# for dataset_f in glob.glob(\"/data/ross/assay_calibration/scoresets/*.json\"):\n",
    "    \n",
    "#     dataset_name = dataset_f.split('/')[-1][:-5]\n",
    "\n",
    "#     for component_range in (\"2-component\",\"3-component\"):\n",
    "#         for monotonicity_contraint in (\"no constraint\",\"constraint\"):\n",
    "#             # for init_strategy in (\"kmeans\",\"random\"):\n",
    "#             init_strategy = \"kmeans\"\n",
    "            \n",
    "#             fits, ds = test_fit(dataset_f, component_range=[3] if component_range[0] == \"3\" else [2], \n",
    "#                                 check_monotonic=False if monotonicity_contraint[0] == \"n\" else True, \n",
    "#                                 init_strategy=init_strategy)\n",
    "            \n",
    "#             fit_results = sorted(fits, key=lambda res: res['likelihoods'][-1], reverse=True)\n",
    "#             best_fit = fit_results[0]\n",
    "#             scores = ds.scores\n",
    "#             sample_assignments = ds.sample_assignments\n",
    "            \n",
    "#             score_range = np.linspace(scores.min(), scores.max(), 1000)\n",
    "#             estimatedDensities = np.array([density_utils.joint_densities(score_range[...,None],\n",
    "#                                                                          best_fit['component_params'],\n",
    "#                                                                          sample_weights).squeeze() for sample_weights in best_fit['weights']])\n",
    "            \n",
    "#             fig,ax = plt.subplots(3,1, figsize=(12,12))\n",
    "#             for i in range(3):\n",
    "#                 ax[i].plot(score_range, estimatedDensities[i].sum(0), label='Estimated', color='C1', linestyle='--')\n",
    "#                 ax[i].hist(scores[sample_assignments[:,i]], density=True, alpha=0.3, color='gray', label='Data histogram')\n",
    "#                 ax[i].set_title(f'Sample {i+1}')\n",
    "#                 ax[i].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KCNH2_O_Neill_2024_surface_expression\n",
      "KCNH2_O_Neill_2024_surface_expression: 68540 total variants\n",
      "\tPathogenic/Likely Pathogenic: 198 variants\n",
      "\tBenign/Likely Benign: 13 variants\n",
      "\tgnomAD: 1243 variants\n",
      "\tSynonymous: 1003 variants\n",
      "\n",
      "  Saved to /data/ross/assay_calibration/test_experimental_plots_v7/KCNH2_O_Neill_2024_surface_expression.png\n",
      "Processing HMBS_van_Loggerenberg_2023_combined\n",
      "HMBS_van_Loggerenberg_2023_combined: 19977 total variants\n",
      "\tPathogenic/Likely Pathogenic: 45 variants\n",
      "\tBenign/Likely Benign: 4 variants\n",
      "\tgnomAD: 387 variants\n",
      "\tSynonymous: 286 variants\n",
      "\n",
      "  Saved to /data/ross/assay_calibration/test_experimental_plots_v7/HMBS_van_Loggerenberg_2023_combined.png\n",
      "All plots saved!\n"
     ]
    }
   ],
   "source": [
    "#### import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/data/ross/assay_calibration/test_experimental_plots_v7\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Suppress matplotlib debug messages\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "n_non_violating_fits_without_constraint = 0\n",
    "\n",
    "for dataset_f in (\n",
    "    glob.glob(\"/data/ross/assay_calibration/scoresets/KCNH2_O_Neill_2024_surface_expression*.json\")\n",
    "    + glob.glob(\"/data/ross/assay_calibration/scoresets/HMBS_van_Loggerenberg_2023_combined*.json\")\n",
    "):\n",
    "    \n",
    "    dataset_name = dataset_f.split('/')[-1][:-5]\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "\n",
    "    ds = Scoreset.from_json(dataset_f)\n",
    "    print(ds)\n",
    "\n",
    "    sample_names = np.array([sample[1] for sample in ds.samples])\n",
    "    \n",
    "    # Create figure with 4x3 subplot grid\n",
    "    fig, axes = plt.subplots(len(sample_names), 4, figsize=(18, 5*len(sample_names)))\n",
    "    fig.suptitle(f'Dataset: {dataset_name}', fontsize=16, y=0.995)\n",
    "\n",
    "    all_results[dataset_name] = {}\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for component_range in [\"2-component\", \"3-component\"]:\n",
    "        for monotonicity_constraint in [\"constraint\", \"no constraint\"]:\n",
    "            # for init_density_fix in [\"lambda\",\"sigma\"]:\n",
    "            init_strategy = \"random\"\n",
    "            \n",
    "            # Run the fit\n",
    "            try:\n",
    "                fits, ds = test_fit(\n",
    "                    ds, \n",
    "                    component_range=[3] if component_range[0] == \"3\" else [2], \n",
    "                    check_monotonic=False if monotonicity_constraint[0] == \"n\" else True, \n",
    "                    init_strategy=init_strategy\n",
    "                )\n",
    "                \n",
    "                # Get best fit\n",
    "                fit_results = sorted(fits, key=lambda res: res['likelihoods'][-1], reverse=True)\n",
    "                best_fit = fit_results[0]\n",
    "                best_init = \"MoM\" if best_fit['kmeans'] == \"method_of_moments\" else \"KM\"\n",
    "                xlims = best_fit['xlims']\n",
    "                scores = ds.scores\n",
    "                sample_assignments = ds.sample_assignments\n",
    "    \n",
    "                \n",
    "                all_results[dataset_name][(component_range, monotonicity_constraint)] = best_fit\n",
    "    \n",
    "                # Calculate densities\n",
    "                score_range = np.linspace(scores.min(), scores.max(), 1000)\n",
    "                estimatedDensities = np.array([\n",
    "                    density_utils.joint_densities(\n",
    "                        score_range[..., None],\n",
    "                        best_fit['component_params'],\n",
    "                        sample_weights\n",
    "                    ).squeeze() for sample_weights in best_fit['weights']\n",
    "                ])\n",
    "                \n",
    "                # Check if density constraint violated\n",
    "                fit_violates_constraint = constraints.multicomponent_density_constraint_violated(best_fit['component_params'], xlims)\n",
    "                if monotonicity_constraint == \"no constraint\" and fit_violates_constraint:\n",
    "                    n_non_violating_fits_without_constraint += 1\n",
    "                fit_violates_constraint = \"violates\" if fit_violates_constraint else \"not violates\"\n",
    "                \n",
    "                # Plot for each sample (3 columns)\n",
    "                for i in range(len(estimatedDensities)):\n",
    "                    ax = axes[i, plot_idx]\n",
    "                    \n",
    "                    # Plot estimated density\n",
    "                    ax.plot(score_range, estimatedDensities[i].sum(0), \n",
    "                           label='Estimated', color='C1', linestyle='-', linewidth=2)\n",
    "                    \n",
    "                    # Plot histogram of actual data\n",
    "                    if i < sample_assignments.shape[1] and sample_assignments[:, i].sum() > 0:\n",
    "                        ax.hist(scores[sample_assignments[:, i]], \n",
    "                               bins=30, density=True, alpha=0.3, \n",
    "                               color='gray', label='Data')\n",
    "                    \n",
    "                    # Set labels and title\n",
    "                    if plot_idx == 0:\n",
    "                        ax.set_ylabel(sample_names[i], \n",
    "                                     fontsize=12, fontweight='bold')\n",
    "                    \n",
    "                    if i == 0:\n",
    "                        ax.set_title(f'{component_range}, {monotonicity_constraint}, {best_init}, {fit_violates_constraint}', fontsize=12)\n",
    "                    \n",
    "                    if i == len(estimatedDensities) - 1:\n",
    "                        ax.set_xlabel('Score', fontsize=10)\n",
    "                    \n",
    "                    ax.legend(loc='upper right', fontsize=8)\n",
    "                    ax.grid(True, alpha=0.2)\n",
    "                    \n",
    "                    # Add likelihood value as text\n",
    "                    likelihood = best_fit['likelihoods'][-1]\n",
    "                    ax.text(0.02, 0.98, f'LL: {likelihood:.3f}, n={len(scores[sample_assignments[:, i]])}', \n",
    "                           transform=ax.transAxes, fontsize=12,\n",
    "                           verticalalignment='top')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {component_range}, {monotonicity_constraint}: {e}\")\n",
    "                # Create empty plots with error message\n",
    "                for i in range(len(estimatedDensities)):\n",
    "                    ax = axes[i, plot_idx]\n",
    "                    ax.text(0.5, 0.5, f'Error:\\n{str(e)[:30]}...', \n",
    "                           ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_xticks([])\n",
    "                    ax.set_yticks([])\n",
    "\n",
    "                    if i == 0:\n",
    "                        ax.set_title(f'{component_range}, {monotonicity_constraint}, {best_init}, {fit_violates_constraint}', fontsize=12)\n",
    "                    \n",
    "                    if plot_idx == 0:\n",
    "                        ax.set_ylabel(sample_names[i], \n",
    "                                     fontsize=12, fontweight='bold')\n",
    "                    \n",
    "                    if i == len(estimatedDensities) - 1:\n",
    "                        ax.set_xlabel('Score', fontsize=10)\n",
    "            \n",
    "            plot_idx += 1\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "    \n",
    "    output_path = f\"{output_dir}/{dataset_name}.png\"\n",
    "    plt.savefig(output_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  Saved to {output_path}\")\n",
    "\n",
    "print(\"All plots saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset_f in glob.glob(\"/data/ross/assay_calibration/scoresets/KCNH2_O_Neill_2024_surface_expression*.json\"):\n",
    "#     dataset_name = dataset_f.split('/')[-1][:-5]\n",
    "#     print(f\"Processing {dataset_name}\")\n",
    "\n",
    "#     ds = Scoreset.from_json(dataset_f)\n",
    "#     print(ds)\n",
    "\n",
    "#     sample_names = np.array([sample[1] for sample in ds.samples])\n",
    "    \n",
    "#     # Create figure with 4x3 subplot grid\n",
    "#     fig, axes = plt.subplots(len(sample_names), 4, figsize=(18, 5*len(sample_names)))\n",
    "#     fig.suptitle(f'Dataset: {dataset_name}', fontsize=16, y=0.995)\n",
    "    \n",
    "#     for i,component_range in enumerate([\"2-component\", \"3-component\"]):\n",
    "#         for monotonicity_constraint in [\"constraint\", \"no constraint\"]:\n",
    "#             if (component_range,monotonicity_constraint) not in all_results[dataset_name]:\n",
    "#                 continue\n",
    "#             best_fit = all_results[dataset_name][(component_range,monotonicity_constraint)]\n",
    "        \n",
    "#             best_init = \"MoM\" if best_fit['kmeans'] == \"method_of_moments\" else \"KM\"\n",
    "#             xlims = best_fit['xlims']\n",
    "#             scores = ds.scores\n",
    "#             sample_assignments = ds.sample_assignments\n",
    "\n",
    "\n",
    "#             # Calculate densities\n",
    "#             score_range = np.linspace(scores.min(), scores.max(), 1000)\n",
    "#             estimatedDensities = np.array([\n",
    "#                 density_utils.joint_densities(\n",
    "#                     score_range[..., None],\n",
    "#                     best_fit['component_params'],\n",
    "#                     sample_weights\n",
    "#                 ).squeeze() for sample_weights in best_fit['weights']\n",
    "#             ])\n",
    "            \n",
    "#             # Check if density constraint violated\n",
    "#             fit_violates_constraint = constraints.multicomponent_density_constraint_violated(best_fit['component_params'], xlims)\n",
    "#             if monotonicity_constraint == \"no constraint\" and fit_violates_constraint:\n",
    "#                 n_non_violating_fits_without_constraint += 1\n",
    "#             fit_violates_constraint = \"violates\" if fit_violates_constraint else \"not violates\"\n",
    "            \n",
    "#             # Plot for each sample (3 columns)\n",
    "#             for i in range(len(estimatedDensities)):\n",
    "#                 ax = axes[i, plot_idx]\n",
    "                \n",
    "#                 # Plot estimated density\n",
    "#                 ax.plot(score_range, estimatedDensities[i].sum(0), \n",
    "#                        label='Estimated', color='C1', linestyle='-', linewidth=2)\n",
    "                \n",
    "#                 # Plot histogram of actual data\n",
    "#                 if i < sample_assignments.shape[1] and sample_assignments[:, i].sum() > 0:\n",
    "#                     ax.hist(scores[sample_assignments[:, i]], \n",
    "#                            bins=30, density=True, alpha=0.3, \n",
    "#                            color='gray', label='Data')\n",
    "                \n",
    "#                 # Set labels and title\n",
    "#                 if plot_idx == 0:\n",
    "#                     ax.set_ylabel(sample_names[i], \n",
    "#                                  fontsize=12, fontweight='bold')\n",
    "                \n",
    "#                 if i == 0:\n",
    "#                     ax.set_title(f'{component_range}, {monotonicity_constraint}, {best_init}, {fit_violates_constraint}', fontsize=12)\n",
    "                \n",
    "#                 if i == len(estimatedDensities) - 1:\n",
    "#                     ax.set_xlabel('Score', fontsize=10)\n",
    "                \n",
    "#                 ax.legend(loc='upper right', fontsize=8)\n",
    "#                 ax.grid(True, alpha=0.2)\n",
    "                \n",
    "#                 # Add likelihood value as text\n",
    "#                 likelihood = best_fit['likelihoods'][-1]\n",
    "#                 ax.text(0.02, 0.98, f'LL: {likelihood:.3f}, n={len(scores[sample_assignments[:, i]])}', \n",
    "#                        transform=ax.transAxes, fontsize=12,\n",
    "#                        verticalalignment='top')\n",
    "            \n",
    "        \n",
    "#         plot_idx += 1\n",
    "\n",
    "# # Adjust layout and save\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-component: 44.4% (36/81) did not violate the constraint.\n",
      "3-component: 12.8% (10/78) did not violate the constraint.\n"
     ]
    }
   ],
   "source": [
    "n_non_violating_fits_without_constraint = [0,0]\n",
    "n_violating_fits_without_constraint = [0,0]\n",
    "for dataset_f in glob.glob(\"/data/ross/assay_calibration/scoresets/*.json\"):\n",
    "    dataset_name = dataset_f.split('/')[-1][:-5]\n",
    "    for i,component_range in enumerate([\"2-component\", \"3-component\"]):\n",
    "        for monotonicity_constraint in [\"constraint\", \"no constraint\"]:\n",
    "            if (component_range,monotonicity_constraint) not in all_results[dataset_name]:\n",
    "                continue\n",
    "            params = all_results[dataset_name][(component_range,monotonicity_constraint)]['component_params']\n",
    "            xlims = all_results[dataset_name][(component_range,monotonicity_constraint)]['xlims']\n",
    "            fit_violates_constraint = constraints.multicomponent_density_constraint_violated(params, xlims)\n",
    "            if monotonicity_constraint == \"no constraint\" and fit_violates_constraint:\n",
    "                n_violating_fits_without_constraint[i] += 1\n",
    "            elif monotonicity_constraint == \"no constraint\":\n",
    "                n_non_violating_fits_without_constraint[i] += 1\n",
    "\n",
    "for i,component_range in enumerate([\"2-component\", \"3-component\"]):\n",
    "    print(f'{component_range}: {100*n_non_violating_fits_without_constraint[i]/(n_non_violating_fits_without_constraint[i]+n_violating_fits_without_constraint[i]):.1f}% ({n_non_violating_fits_without_constraint[i]}/{n_non_violating_fits_without_constraint[i]+n_violating_fits_without_constraint[i]}) did not violate the constraint.')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = 0\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = Scoreset.from_json('/data/ross/assay_calibration/scoresets/BAP1_Waters_2024.json')\n",
    "# constraints.multicomponent_density_constraint_violated(\n",
    "#     all_results['BAP1_Waters_2024'][('3-component','constraint')]['component_params'],\n",
    "#     (ds.scores.min(), ds.scores.max())\n",
    "# ), all_results['BAP1_Waters_2024'][('3-component','constraint')]['component_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HMBS_van_Loggerenberg_2023_combined', 'TP53_Fortuno_2021_Kato_meta' truth value of arr\n",
    "# 'KCNH2_Jiang_2022', 'KCNH2_O_Neill_2024_surface_expression' ll decreased consistently & xlims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints.multicomponent_density_constraint_violated(best_fit['component_params'],\n",
    "                                                       score_range[[0,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['component_params', 'weights', 'likelihoods', 'history', 'kmeans'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_fit.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-8.553172420104763, 0.7949678509378952, 0.24375170761527565),\n",
       " (-1.4489019235176928, 0.9296749533938731, 0.04216918377139136),\n",
       " (-7.860341896888932e-05, 1.003620376681616, 0.020770124957920067)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_fit['component_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.483185729440366), np.float64(1.36169269063829))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.min(),scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration 9618: Likelihood DECREASED by 1.96e-06 (relative: 3.51e-06) - algorithmic issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (assay_calibration)",
   "language": "python",
   "name": "assay_calibration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
